{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "data_pre = np.load('stft_feature_matrix_pre.npy')\n",
    "data_inter = np.load('stft_feature_matrix_inter.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pre = data_pre.reshape(22 * 641, 12607).T\n",
    "data_inter = data_inter.reshape(22 * 641, 12607).T\n",
    "\n",
    "#转为时间，频率*通道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建标签\n",
    "labels_pre = np.ones(data_pre.shape[0])   # 癫痫前期为1\n",
    "labels_inter = np.zeros(data_inter.shape[0])  # 癫痫间期为0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并数据标签\n",
    "data = np.concatenate((data_pre, data_inter), axis=0)\n",
    "labels = np.concatenate((labels_pre, labels_inter), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集30%，验证集15%，测试集15%\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "channels = 22\n",
    "frequency_points = 641\n",
    "\n",
    "X_train = X_train.reshape(-1, channels, frequency_points, 1)\n",
    "X_val = X_val.reshape(-1, channels, frequency_points, 1)\n",
    "X_test = X_test.reshape(-1, channels, frequency_points, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#保存划分数据\n",
    "np.save('X_train.npy', X_train)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('X_val.npy', X_val)\n",
    "np.save('y_val.npy', y_val)\n",
    "np.save('X_test.npy', X_test)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#读取划分数据\n",
    "X_train = np.load('X_train.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "X_val = np.load('X_val.npy')\n",
    "y_val = np.load('y_val.npy')\n",
    "X_test = np.load('X_test.npy')\n",
    "y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridCNNTransformer(\n",
      "  (cnn_unit): CNNLayer(\n",
      "    (units): Sequential(\n",
      "      (0): CNNUnit(\n",
      "        (conv): Conv2d(22, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (1): CNNUnit(\n",
      "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "      (2): CNNUnit(\n",
      "        (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cnn1): CNNLayer(\n",
      "    (units): Sequential(\n",
      "      (0): CNNUnit(\n",
      "        (conv): Conv2d(16, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer1): TransformerBlock(\n",
      "    (transformer): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=24, out_features=24, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=24, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=24, bias=True)\n",
      "      (norm1): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((24,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (cnn2): CNNLayer(\n",
      "    (units): Sequential(\n",
      "      (0): CNNUnit(\n",
      "        (conv): Conv2d(24, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "        (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (transformer2): TransformerBlock(\n",
      "    (transformer): TransformerEncoderLayer(\n",
      "      (self_attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (linear1): Linear(in_features=32, out_features=2048, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (linear2): Linear(in_features=2048, out_features=32, bias=True)\n",
      "      (norm1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout1): Dropout(p=0.1, inplace=False)\n",
      "      (dropout2): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (avg_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc1): Linear(in_features=32, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class CNNUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(CNNUnit, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_units, stride=1):\n",
    "        super(CNNLayer, self).__init__()\n",
    "        units = []\n",
    "        for _ in range(num_units):\n",
    "            units.append(CNNUnit(in_channels, out_channels, stride))\n",
    "            in_channels = out_channels  # 更新输入通道数\n",
    "        self.units = nn.Sequential(*units)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.units(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, feature_size, num_heads, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.transformer = nn.TransformerEncoderLayer(\n",
    "            d_model=feature_size,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.transformer(x)\n",
    "\n",
    "class HybridCNNTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridCNNTransformer, self).__init__()\n",
    "        # CNN Unit (assuming input channels is 1 for STFT)\n",
    "        self.cnn_unit = CNNLayer(in_channels=channels, out_channels=16, num_units=3)\n",
    "        \n",
    "        # CNN and Transformer layers\n",
    "        self.cnn1 = CNNLayer(in_channels=16, out_channels=24, num_units=1, stride=2)\n",
    "        self.transformer1 = TransformerBlock(feature_size=24, num_heads=8, dropout_rate=0.1)\n",
    "        \n",
    "        self.cnn2 = CNNLayer(in_channels=24, out_channels=32, num_units=1, stride=2)\n",
    "        self.transformer2 = TransformerBlock(feature_size=32, num_heads=8, dropout_rate=0.1)\n",
    "        \n",
    "        # Pooling layer\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initial CNN processing\n",
    "        x = self.cnn_unit(x)\n",
    "        \n",
    "        # First CNN and Transformer stage\n",
    "        x = self.cnn1(x)\n",
    "        x = x.flatten(2)  # Flatten the CNN features for the transformer\n",
    "        x = x.permute(2, 0, 1)  # Reshape for transformer (Seq, Batch, Feature)\n",
    "        x = self.transformer1(x)\n",
    "        x = x.permute(1, 2, 0).unsqueeze(-1)  # Reshape back (Batch, Feature, Seq, 1)\n",
    "        \n",
    "        # Second CNN and Transformer stage\n",
    "        x = self.cnn2(x)\n",
    "        x = x.flatten(2)  # Flatten the CNN features for the transformer\n",
    "        x = x.permute(2, 0, 1)  # Reshape for transformer\n",
    "        x = self.transformer2(x)\n",
    "        x = x.permute(1, 2, 0).unsqueeze(-1)  # Reshape back\n",
    "        \n",
    "        # Pooling and fully connected layers\n",
    "        x = self.avg_pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "model = HybridCNNTransformer()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据转换为PyTorch张量\n",
    "train_data = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_data = TensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 初始化模型、损失函数和优化器\n",
    "model = HybridCNNTransformer()\n",
    "criterion = nn.CrossEntropyLoss()  # 用于分类问题\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 定义训练函数\n",
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs):\n",
    "    best_val_loss = float('inf')  # 初始化最佳验证损失\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            # 清空优化器\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向传播和优化\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "        # 验证阶段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # 计算平均验证损失\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Validation Loss: {avg_val_loss}')\n",
    "        \n",
    "        # 如果这个epoch的验证损失是迄今为止最佳的，保存模型\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')  # 保存模型参数\n",
    "            print('Model saved')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 训练模型\n",
    "epochs = 4\n",
    "print(\"start\")\n",
    "train(model, criterion, optimizer, train_loader, val_loader, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮25分钟左右\n",
    "第二轮36min\n",
    "\n",
    "```\n",
    "Epoch 1/2 - Loss: 0.5830323612992314\n",
    "Epoch 1/2 - Validation Loss: 0.830383938550949\n",
    "Epoch 2/2 - Loss: 0.5181694445402726\n",
    "Epoch 2/2 - Validation Loss: 0.666413692633311\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
